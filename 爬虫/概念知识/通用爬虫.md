## 通用爬虫

### 通用爬虫CrawlSpider

```python
import scrapy.spider import CrawlSpider,Rule
from scrapy.linkextractors import LinkExtractor

class DushuSpider(CrawlSpider):
    name = 'dushu',
    allowed_domains = ['www.dushu.com']
    start_urls = ['http://www.dushu.com/']
    
    rules = (
    	Rule(LinkExtractor(allow=r'/guoxue/\d+'),follow=True),
        Rule(LinkExtractor(allow=r'/guoxue/\d+'),callback="parse_item"),
    )
    
    def parse_item(self,response):
        print(response.xpath('//title/text()'))
  ................................................................


#与scrapy爬虫相比，crawlspider使用rules规则，相当于scrapy爬虫中的：
def parse_link(self,response):
    links = response.xpath('//a/@href')
    for link in links:
        yield scrapy.Request(link,callback=self.parse_item)
        
def parse_item(self,response):
        print(response.xpath('//title/text()'))
```

