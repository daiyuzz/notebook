## 多线程分布式爬虫

```python
”“”
爬虫：
	for url in urls:
    url发送请求-》获得response-》保存数据

多线程爬虫：
	urls 保存在本地内存
    work（url ->发送请求->获得Response->解析Response->保存数据
    启用多个work
    
多线程分布式爬虫：
	urls保存在redis内存数据库中
    多台电脑从redis内存数据库取url
    每台电脑执行的操作为 work(url ->获得Response ->解析Response ->保存数据)
“”“”
```



```python
import threading
from redis import Redis
import time
import sys

IDLE = 0 #空闲状态
WORKING = 1 #工作作态
REDIS_SPIDER_URLS_KEY = 'spider:urls'
start_url = 'http://m.sogou.com/'
ALLOW_DOMAIN = 'sogou.com'
MAX_DOWNLOAD_THREAD = 10  #并发线程数
RETRY_TIME = 3
DOWNLOAD_DELAY = 5

crawled_url = set() #爬取过的url
REDIS_CRAWLED_URL = 'crawled_url'

rds = Redis('127.0.0.1',6379)


def fetch(url):
    """
    下载页面，如果下载成功，返回网页源码---str
    """
    #如果是新的url，就下载url对应的页面，如果url已经爬取过，忽略
    if rds.sadd(REDIS_CRAWLED_URL,url) ==1:
        print('start fetching .. {url}'.format(url=url))
        print(f'Start fetching ..{url}')
        resp = requests.get(url)
        if resp.status_code == 200:
            return resp.text
        return None


def parse(html):
    """
    解析html，从html中提取所有的url
    返回list类型的url列表
    """
    if html is None:
        return None
    tree = etree.HTML(html)
    urls = tree.xpath('//a/@href')
    new_urls =[]
    for url in urls:
        if ALLOW_DOMAIN in url:
            if url.startswith('http') or url.startswith('https'):
                new_urls.append(url)
    if not new_urls:
        return None
    return new_urls

class Spider(threading.Thread):
    """爬虫器"""
    def __init__(self):
        super(Spider,self).__init__()
        self.status = IDLE
    
    @property
    def is_idle(self):
        """
        判断对象是不是空闲状态，如果空闲，返回True
        """
        return self.status == IDLE
    
    
    def run(self):
        while True:
            #从redis_url队列中取出一个url，如果队列中的暂时没有url，等待
            url = rds.blpop(REDIS_SPIDER_URLS_KEY)[1]
            
            #开始爬取url对应的页面
            self.status = WORKING
            #获取网页源码
            html = fetch(url)
            urls = parse(html)
            if urls is not None:
                new_urls = set(urls) #获取页面所有的url，并去重
                rds.lpush(REDIS_SPIDER_URLS_KEY,*new_urls)
                
             #线程爬取结束
            self.status = IDLE   #设置为空闲状态

def all_is_idle(spiders):
    """
    检测所有的爬虫线程是否均为空闲状态，如果都是空闲状态返回True，否则返回False
    """
    spiders_is_idle = [spider.is_idle for spider in spiders]
    return all(spiders_is_idle)  #all()有一个为假结果为假
    
    #第二种方法：
    #for spider in spiders:
        #if spider.is_idle is not True:
            #return False
        #return True
            
                
def main(n):
    #创建n个线程
    rds.lpush(REDIS_SPIDER_URLS_KEY,start_url)
    spider_pools =[]
    for i in range(n):
        spider = Spider()
        spider_pools.append(spider)
        
    for spider in spider_pools:
        spider.start()
        
    while True:
        #监听rds中的待爬取队列
        #如果待爬取队列为空，结束爬虫
        if rds.llen(REDIS_SPIDER_URLS_KEY) ==0 and all_is_idle(spider_pools):
            print('所有线程爬取完毕')
            sys.exit(0)
        else:
            time.sleep(2)
            
if __name__ =="__main__":
    main(MAX_DOWNLOAD_THREAD)
    
            

        
        
    

```



```python
from redis import Redis
import time

rds = Redis('127.0.0.1',port=6379)
while True:
    #实时监控redis中的’spider:item‘键，每隔2秒提取一条数据
    
    key,value = rds.blpop('spider:items')
    time.sleep(2)
    #把value保存到其它数据库.....
    ...
```

