## 爬取商品

```python
#items.py

from scrapy import Field, Item


class ParameterItem(Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    _id = Field()
    item_name = Field()
    sku_id = Field()
    parameters1 = Field()
    parameters2 = Field()
    name = Field()
    price = Field()
    brand = Field()
```



```python
#jd.py

import json
import logging
import random
import uuid
from urllib.parse import urlparse

import scrapy
from scrapy import Spider

from jd.items import ParameterItem

from jd.jd.spiders.exception import ParseNotSupportedError

logger = logging.getLogger('jindong')


class JDSpider(scrapy.Spider):
    name = "jindong"
    rotete_user_agent = True  #轮转user_agent

    # 在关闭爬虫的之前，保存资源

    def __init__(self):
        self.price_url = "https://p.3.cn/prices/mgets?pduid={}&skuIds=J_{}"
        self.price_backup_url = "https://p.3.cn/prices/get?pduid={}&skuid=J_{}"
        self.jd_subdomain = ["jiadian", "shouji", "wt", "shuma", "diannao",
                             "bg", "channel", "jipiao", "hotel", "trip",
                             "ish", "book", "e", "health", "baby", "toy",
                             "nong", "jiu", "fresh", "china", "che", "list"]

    def start_requests(self):
        urls = ['https://jd.com/']
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        links = response.xpath("//@href").extract()
        for link in links:
            self.logger.debug("link: %s", link)   #logger.debug配合log的等级过滤输出
            url = urlparse(link)    #将link进行拆解，结果存放在url中
            if url.netloc:  # 是一个 URL 而不是 "javascript:void(0)" （服务器位置也有可能是用户信息） 例：bao.jd.com
                if not url.scheme:  #url.scheme为http
                    link = "http:" + link
                self.logger.debug("url: %s", url)
                subdomain = url.hostname.split(".")[0] #hosename服务器名称或地址  例：bao
                # 如果这是一个商品，获取商品详情
                if subdomain in ["item"]:   #???
                    yield scrapy.Request(url=link, callback=self.parse_item)
                # 判断是否是 xxx.jd.com, 限制爬取范围在 jd.com
                elif subdomain in self.jd_subdomain:
                    yield scrapy.Request(url=link, callback=self.parse)

    def parse_item(self, response):
        if not response.text or response.text == "":
            return
        parsed = urlparse(response.url)
        item = ParameterItem()
        # 获取 sku_id, URL: http://item.jd.com/4297772.html 的 sku_id 就是 4297772  ,path就是/4297772.html
        item['sku_id'] = urlparse(response.url).path.split(".")[
            0].replace("/", "")
        item['brand'] = response.xpath(
            "id('parameter-brand')/li/a[1]/text()").extract()
        item['_id'] = str(uuid.uuid4()) #通过随机数来生成UUID
        names = response.xpath('//div[@class="sku-name"]/text()').extract()
        name = ''.join(str(i) for i in names).strip()
        if name == '':
            name = response.xpath("id('name')/h1/text()").extract()
        item['name'] = name
        item['item_name'] = 'parameter'
        details2 = []
        # 不同的子域名对应的页面解析规则不一样
        if parsed.hostname != 'e.jd.com':
            # 通用的解析策略
            details1 = response.xpath(
                '//ul[@class="parameter1 p-parameter-list"]/li/div/p/text()').extract()
            item['parameters1'] = details1
            details2 = response.xpath(
                '//ul[@class="parameter2 p-parameter-list"]/li/text()').extract()
            if details2 == []:
                try:
                    # 解析图书
                    details2 = self.parse_book(response)
                except ParseNotSupportedError as err:
                    # 解析全球购
                    parsed_url = urlparse(response.url)
                    subdomain = parsed_url.hostname
                    if subdomain == "item.jd.hk":
                        logging.info("parse global shopping item, url: {}"
                                     .format(response.url))
                        details2 = self.parse_global_shopping(response)
                    else:
                        # 无法解析
                        referer = response.request.headers.get('Referer', None)
                        logging.warn("Unknow item, could not parse. refrece: {}, url: {}"
                                     .format(referer, response.url))
        item['parameters2'] = details2
        yield scrapy.Request(url=self.price_url.format(random.randint(1, 100000000), item['sku_id']),
                             callback=self.parse_price,
                             meta={'item': item})

    def parse_price(self, response):
        item = response.meta['item']
        try:
            price = json.loads(response.text)
            item['price'] = price[0].get("p")
        except KeyError as err:
            if price['error'] == 'pdos_captcha':
                logging.error("触发验证码")
            logging.warn("Price parse error, parse is {}".format(price))
        except json.decoder.JSONDecodeError as err:
            logging.log('prase price failed, try backup price url')
            yield scrapy.Request(url=self.price_backup_url.format(random.randint(1, 100000000),
                                                                  item['sku_id']),
                                 callback=self.parse_price,
                                 meta={'item': item})
        yield item

    def parse_global_shopping(self, response):
        try:
            details2 = []
            brand = {}
            brand_name = response.xpath(
                "id('item-detail')/div[1]/ul/li[3]/text()").extract()[0]
            brand[brand_name] = response.xpath(
                "id('item-detail')/div[1]/ul/li[3]/a/text()").extract()[0]
            details2.append(brand)
            parameters = response.xpath(
                "id('item-detail')/div[1]/ul/li/text()").extract()
            details2 += parameters[0:2]
            details2 += parameters[3:]
            return details2
        except IndexError as err:
            referer = response.request.headers.get('Referer', None)
            logging.warn("global shopping parses failed. referer: {}, url: {}".format(
                referer, response.url))

    def parse_book(self, response):
        try:
            details2 = []
            shop = {}
            shop_name = response.xpath(
                "id('parameter2')/li[1]/text()").extract()[0]
            shop[shop_name] = response.xpath(
                "id('parameter2')/li[1]/a/text()").extract()[0]
            details2.append(shop)
            publisher = {}
            publisher_name = response.xpath(
                "id('parameter2')/li[2]/text()").extract()[0]
            publisher[publisher_name] = response.xpath(
                "id('parameter2')/li[2]/a/text()").extract()
            details2.append(publisher)
            details2 += response.xpath(
                "id('parameter2')/li/text()").extract()[3:]
            return details2
        except IndexError as err:
            referer = response.request.headers.get('Referer', None)
            logging.info("It's not a book item, parse failed. referer: {}, url: {}"
                         .format(referer, response.url))
            raise ParseNotSupportedError(response.url)
```



```python
#exception.py

class ParseNotSupportedError(Exception):
    def __init__(self, url):
        self.url = url

    def __str__(self):
        return 'url {} is could not be parsed '.format(self.url)

```



```python
#轮转user-agent中间件
#middlewares.py
# -*- coding: utf-8 -*-

# Define here the models for your spider middleware
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/spider-middleware.html

from random import choice

from scrapy import signals
from scrapy.exceptions import NotConfigured


class RotateUserAgentMiddleware(object):
    """Rotate user-age for each request
    """

    def __init__(self, user_agents):
        self.enabled = False
        self.user_agents = user_agents

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        # s = cls()
        # crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        # return s
        user_agents = crawler.settings.get('USER_AGENT_CHOICES', [])

        if not user_agents:
            raise NotConfigured("USER_AGENT_CHOICES not set or empty")

        o = cls(user_agents)
        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
        return o

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, dict or Item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method
        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Response, dict
        # or Item objects.
        pass

    def process_start_requests(self, start_requests, spider):
        # Called with the start requests of the spider, and works
        # similarly to the process_spider_output() method, except
        # that it doesn’t have a response associated.

        # Must return only requests (not items).
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)
        self.enabled = getattr(spider, 'rotate_user_agent', self.enabled)

    def process_request(self, request, spider):
        if not self.enabled or not self.user_agents:
            return
        request.headers['user-agent'] = choice(self.user_agents)

```



```python
#pipelines.py

# -*- coding: utf-8 -*-


import logging
import os

# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html
import pymongo
import redis
# Define your item pipelines here
#
from scrapy.conf import settings
from scrapy.exceptions import DropItem



class MongoDBPipeline(object):
    def __init__(self):
        pool = redis.ConnectionPool(
            host=settings['REDIS_HOST'], port=settings['REDIS_PORT'], db=0)
        self.server = redis.Redis(connection_pool=pool)
        connection = pymongo.MongoClient(
            settings['MONGODB_SERVER'],
            settings['MONGODB_PORT']
        )
        self.db = connection[settings['MONGODB_DB']]

    def process_item(self, item, spider):
        valid = True
        for data in item:
            if not data:
                valid = False
                raise DropItem("Missing {0}!".format(data))
        if valid:
            try:
                # This returns the number of values added, zero if already exists.
                key = "good_filter_" + item['sku_id']
                item_name = item['item_name']
                if self.server.get(key) is None:
                    self.server.set(key, item_name)
                    self.db[item_name].insert(dict(item))
                    logging.debug("add {}".format(item['item_name']))
            except (pymongo.errors.WriteError, KeyError) as err:
                raise DropItem("Duplicated Item: {}".format(item['name']))
        return item

```



```python
#setting.py
# -*- coding: utf-8 -*-

# Scrapy settings for jd project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     http://doc.scrapy.org/en/latest/topics/settings.html
#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html
#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html

BOT_NAME = 'jd'

SPIDER_MODULES = ['jd.spiders']
NEWSPIDER_MODULE = 'jd.spiders'


# Crawl responsibly by identifying yourself (and your website) on the user-agent
USER_AGENT_CHOICES = [
    'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
    'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',
    'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',
    'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',
    'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)',
    'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',
    'ia_archiver (+http://www.alexa.com/site/help/webmasters; crawler@alexa.com)',
]
# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# Configure maximum concurrent requests performed by Scrapy (default: 16)
CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
# DOWNLOAD_DELAY = 1.5
# The download delay setting will honor only one of:
CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
#COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
# DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
# }

# Enable or disable spider middlewares
# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html
# SPIDER_MIDDLEWARES = {
#    'jd.middlewares.JdSpiderMiddleware': 543,
# }

# Enable or disable downloader middlewares
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html
DOWNLOADER_MIDDLEWARES = {
    'jd.middlewares.RotateUserAgentMiddleware': 110,
}

# Enable or disable extensions
# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html
# EXTENSIONS = {
#    'scrapy.extensions.telnet.TelnetConsole': None,
# }

# Configure item pipelines
# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
    'jd.pipelines.MongoDBPipeline': 300,
    'scrapy_redis.pipelines.RedisPipeline': 300
}
MONGODB_SERVER = "localhost"
MONGODB_PORT = 27017
MONGODB_DB = "jindong"

# Enable and configure the AutoThrottle extension (disabled by default)
# See http://doc.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = 'httpcache'
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
# Retry many times since proxies often fail
# RETRY_TIMES = 10
# # Retry on most error codes since proxies fail for different reasons
# RETRY_HTTP_CODES = [500, 503, 504, 400, 403, 404, 408]

# DOWNLOADER_MIDDLEWARES = {
#     'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,
#     'scrapy_proxies.RandomProxy': 100,
#     'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,
# }

# Proxy list containing entries like
# http://host1:port
# http://username:password@host2:port
# http://host3:port
# ...
# PROXY_LIST = './proxy_ip.txt'

# Proxy mode
# 0 = Every requests have different proxy
# 1 = Take only one proxy from the list and assign it to every requests
# 2 = Put a custom proxy to use in the settings
# PROXY_MODE = 0

# If proxy mode is 2 uncomment this sentence :
#CUSTOM_PROXY = "http://host1:port"
# Enables scheduling storing requests queue in redis.
SCHEDULER = "scrapy_redis.scheduler.Scheduler"

# # Ensure all spiders share same duplicates filter through redis.
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"

# DEPTH_LIMIT = 1

REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_DB = "jd_good_filter"

STATS_CLASS = 'jd.statscol.graphite.RedisGraphiteStatsCollector'
GRAPHITE_HOST = '127.0.0.1'
GRAPHITE_PORT = 2003
STATS_KEY = 'scrapy:jd:stats'

```

