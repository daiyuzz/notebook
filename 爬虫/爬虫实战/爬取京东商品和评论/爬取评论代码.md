## 爬取评论代码

```python
#items.py

# -*- coding: utf-8 -*-

# Define here the models for your scraped items

# See documentation in:
# http://doc.scrapy.org/en/latest/topics/items.html

from scrapy import Field, Item


class CommentItem(Item):
    sku_id = Field()
    _id = Field()                 # id
    good_name = Field()
    item_name = Field()
    comment_id = Field()        # 评论 ID
    content = Field()           # 评论内容
    creation_time = Field()       # 评论创建的时间
    reply_count = Field()       # 回复的数量
    score = Field()             # 评分
    useful_vote_count = Field()  # 觉得该评论有用的投票
    useless_vote_count = Field()  # 觉得该评论无用的投票
    user_level_id = Field()       # 用户等级 id
    user_province = Field()       # 用户所在省份
    nickname = Field()           # 用户昵称
    user_level_name = Field()     # 用户等级名
    user_client = Field()         # 用户评论的平台的类型
    user_client_show = Field()    # 用户评论的平台的名称
    is_mobile = Field()           # 是否是来自移动端
    days = Field()                # 是否
    reference_time = Field()
    after_days = Field()
    after_user_comment = Field()


class CommentSummaryItem(Item):
    sku_id = Field()
    _id = Field()
    item_name = Field()
    content = Field()
    good_name = Field()   # 商品的名称
    average_score = Field()       # 平均得分
    good_count = Field()          # 好评数
    general_count = Field()       # 中评数
    poor_count = Field()          # 差评数
    after_count = Field()         # 追评数
    good_rate = Field()           # 好评率
    general_rate = Field()        # 中评率
    poor_rate = Field()           # 差评率

```



```python
#db.py
#!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# author:Samray <samrayleung@gmail.com>

import pymongo
from scrapy.conf import settings


def init_mongodb():
    connection = pymongo.MongoClient(
        settings['MONGODB_SERVER'],
        settings['MONGODB_PORT']
    )
    db = connection[settings['MONGODB_DB']]
    return db

```



```python
#jd_comments_spider.py

#!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# author:Samray <samrayleung@gmail.com>
import json
import logging
import uuid

import scrapy

from jd_comment.db import init_mongodb
from jd_comment.items import CommentItem, CommentSummaryItem


class JDCommentSpider(scrapy.Spider):
    name = 'jd_comment'

    def __init__(self):
        self.comment_url = "https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv118416&productId={}&score=0&sortType=5&page={}&pageSize=10"
        self.db = init_mongodb()
        self.item_db_parameter_name = "parameter"

    def start_requests(self):
        goods = self.get_item_sku_id()
        for sku_id, good_name in goods:
            yield scrapy.Request(url=self.comment_url.format(sku_id, 1),
                                 callback=self.parse_comment,
                                 meta={'page': 1,
                                       'sku_id': sku_id,
                                       'good_name': good_name})

    def parse_comment(self, response):
        comment_item = CommentItem()
        page = response.meta['page']
        sku_id = response.meta['sku_id']
        good_name = response.meta['good_name']
        content = response.text
        # 可能评论为空
        if content != '':
            #将content转换为字典类型
            content = content.replace(r"fetchJSON_comment98vv118416(", "")
            content = content.replace(r");", "")
            data = json.loads(content)

            max_page = data.get("maxPage")
            comments = data.get("comments")
            comment_item['good_name'] = good_name
            comment_item['item_name'] = 'comment'
            comment_item['_id'] = str(uuid.uuid4())
            for comment in comments:
                comment_item['sku_id'] = sku_id
                comment_item['comment_id'] = comment.get('id')  # 评论的 id
                comment_item['content'] = comment.get('content')  # 评论的内容
                comment_item['creation_time'] = comment.get(
                    'creationTime', '')  # 评论创建的时间
                comment_item['reply_count'] = comment.get(
                    'replyCount', '')  # 回复数量

                comment_item['score'] = comment.get('score', '')  # 评星
                comment_item['useful_vote_count'] = comment.get(
                    'usefulVoteCount', '')  # 其他用户觉得有用的数量
                comment_item['useless_vote_count'] = comment.get(
                    'uselessVoteCount', '')  # 其他用户觉得无用的数量
                comment_item['user_level_id'] = comment.get(
                    'userLevelId', '')  # 评论用户等级的 id
                comment_item['user_province'] = comment.get(
                    'userProvince', '')  # 用户的省份
                comment_item['nickname'] = comment.get(
                    'nickname', '')  # 评论用户的昵称
                comment_item['user_level_name'] = comment.get(
                    'userLevelName', '')  # 评论用户的等级
                comment_item['user_client'] = comment.get(
                    'userClient', '')  # 用户评价平台
                comment_item['user_client_show'] = comment.get(
                    'userClientShow', '')  # 用户评价平台
                comment_item['is_mobile'] = comment.get(
                    'isMobile', '')  # 是否是在移动端完成的评价
                comment_item['days'] = comment.get('days', '')  # 购买后评论的天数
                comment_item['reference_time'] = comment.get(
                    'referenceTime', '')  # 购买的时间
                comment_item['after_days'] = comment.get(
                    'afterDays', '')  # 购买后再次评论的天数
                after_user_comment = comment.get('afterUserComment', '')
                if after_user_comment != '' and after_user_comment is not None:
                    h_after_user_comment = after_user_comment.get(
                        'hAfterUserComment', '')
                    after_content = h_after_user_comment.get(
                        'content', '')  # 再次评论的内容
                    comment_item['after_user_comment'] = after_content
                yield comment_item
            if page < max_page:
                yield scrapy.Request(url=self.comment_url.format(sku_id, page + 1),
                                     callback=self.parse_comment,
                                     meta={'page': page + 1, 'sku_id': sku_id,
                                           'good_name': good_name
                                           })
            elif page >= max_page:
                summary_item = CommentSummaryItem()
                comment_summary = data.get("productCommentSummary")
                summary_item['item_name'] = 'comment_summary'
                summary_item['poor_rate'] = comment_summary.get('poorRate')
                summary_item['good_rate'] = comment_summary.get('goodRate')
                summary_item['good_count'] = comment_summary.get('goodCount')
                summary_item['general_count'] = comment_summary.get(
                    'generalCount')
                summary_item['poor_count'] = comment_summary.get('poorCount')
                summary_item['after_count'] = comment_summary.get('afterCount')
                summary_item['average_score'] = comment_summary.get(
                    'averageScore')
                summary_item['sku_id'] = sku_id
                summary_item['good_name'] = good_name
                summary_item['_id'] = str(uuid.uuid4())
                yield summary_item

    def parse_comment_json(self, content):
        data = json.loads(content)
        comments = data.get("comments")
        for comment in comments:
            item = CommentItem()
            item['comment_id'] = comment.get('id')  # 评论的 id
            item['content'] = comment.get('content')  # 评论的内容
            item['creation_time'] = comment.get(
                'creationTime', '')  # 评论创建的时间
            item['reply_count'] = comment.get('replyCount', '')  # 回复数量

            item['score'] = comment.get('score', '')  # 评星
            item['useful_vote_count'] = comment.get(
                'usefulVoteCount', '')  # 其他用户觉得有用的数量
            item['useless_vote_count'] = comment.get(
                'uselessVoteCount', '')  # 其他用户觉得无用的数量
            item['user_level_id'] = comment.get(
                'userLevelId', '')  # 评论用户等级的 id
            item['user_province'] = comment.get('userProvince', '')  # 用户的省份
            item['nickname'] = comment.get('nickname', '')  # 评论用户的昵称
            item['user_level_name'] = comment.get(
                'userLevelName', '')  # 评论用户的等级
            item['user_client'] = comment.get('userClient', '')  # 用户评价平台
            item['user_client_show'] = comment.get(
                'userClientShow', '')  # 用户评价平台
            item['is_mobile'] = comment.get('isMobile', '')  # 是否是在移动端完成的评价
            item['days'] = comment.get('days', '')  # 购买后评论的天数
            item['reference_time'] = comment.get(
                'referenceTime', '')  # 购买的时间
            item['after_days'] = comment.get('afterDays', '')  # 购买后再次评论的天数
            after_user_comment = comment.get('afterUserComment', '')
            if after_user_comment != '' and after_user_comment is not None:
                h_after_user_comment = after_user_comment.get(
                    'hAfterUserComment', '')
                after_content = h_after_user_comment.get(
                    'content', '')  # 再次评论的内容
                item['after_user_comment'] = after_content
                yield item

    def get_item_sku_id(self):
        item_parameters = self.db[self.item_db_parameter_name].find({})
        parameters = [parameter for parameter in item_parameters]
        sku_ids = []
        good_names = []
        for parameter in parameters:
            sku_id = dict(parameter).get("sku_id", None)
            good_name = dict(parameter).get("name", None)
            if good_name:
                good_name = good_name[0]
            else:
                good_name = ''
            sku_ids.append(sku_id)
            good_names.append(good_name)
        good = set(list(zip(sku_ids, good_names)))
        return list(good)


```



```python
#轮转user-agent中间件
#middlewares.py
# -*- coding: utf-8 -*-

# Define here the models for your spider middleware
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/spider-middleware.html

from random import choice

from scrapy import signals
from scrapy.exceptions import NotConfigured


class RotateUserAgentMiddleware(object):
    """Rotate user-age for each request
    """

    def __init__(self, user_agents):
        self.enabled = False
        self.user_agents = user_agents

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        # s = cls()
        # crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        # return s
        user_agents = crawler.settings.get('USER_AGENT_CHOICES', [])

        if not user_agents:
            raise NotConfigured("USER_AGENT_CHOICES not set or empty")

        o = cls(user_agents)
        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
        return o

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, dict or Item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method
        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Response, dict
        # or Item objects.
        pass

    def process_start_requests(self, start_requests, spider):
        # Called with the start requests of the spider, and works
        # similarly to the process_spider_output() method, except
        # that it doesn’t have a response associated.

        # Must return only requests (not items).
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)
        self.enabled = getattr(spider, 'rotate_user_agent', self.enabled)

    def process_request(self, request, spider):
        if not self.enabled or not self.user_agents:
            return
        request.headers['user-agent'] = choice(self.user_agents)

```



```python
#pipeline.py
# -*- coding: utf-8 -*-

import logging

import pymongo
# Define your item pipelines here
#
from scrapy.conf import settings
from scrapy.exceptions import DropItem

from jd_comment.db import init_mongodb


# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html


class MongoDBPipeline(object):
    def __init__(self):
        self.db = init_mongodb()

    def process_item(self, item, spider):
        valid = True
        for data in item:
            if not data:
                valid = False
                raise DropItem("Missing {0}!".format(data))
        if valid:
            try:
                key = {}
                self.db[item['item_name']].insert(dict(item))
                logging.debug("add {}".format(item['item_name']))
            except (pymongo.errors.WriteError, KeyError) as err:
                raise DropItem(
                    "Duplicated comment Item: {}".format(item['good_name']))
        return item

```



```python
# -*- coding: utf-8 -*-

# Scrapy settings for jd_comment project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     http://doc.scrapy.org/en/latest/topics/settings.html
#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html
#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html

BOT_NAME = 'jd_comment'

SPIDER_MODULES = ['jd_comment.spiders']
NEWSPIDER_MODULE = 'jd_comment.spiders'


# Crawl responsibly by identifying yourself (and your website) on the user-agent
#USER_AGENT = 'jd_comment (+http://www.yourdomain.com)'

USER_AGENT_CHOICES = [
    'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
    'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',
    'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',
    'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',
    'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)',
    'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',
    'ia_archiver (+http://www.alexa.com/site/help/webmasters; crawler@alexa.com)',
]
# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
#COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
# DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
#}

# Enable or disable spider middlewares
# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html
# SPIDER_MIDDLEWARES = {
#    'jd_comment.middlewares.JdCommentSpiderMiddleware': 543,
#}

# Enable or disable downloader middlewares
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html
DOWNLOADER_MIDDLEWARES = {
    'jd_comment.middlewares.RotateUserAgentMiddleware': 100,
}

# Enable or disable extensions
# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html
# EXTENSIONS = {
#    'scrapy.extensions.telnet.TelnetConsole': None,
#}

# Configure item pipelines
# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
    'jd_comment.pipelines.MongoDBPipeline': 200,
    'scrapy_redis.pipelines.RedisPipeline': 300,
}
MONGODB_SERVER = "localhost"
MONGODB_PORT = 27017
MONGODB_DB = "jindong"

# Enable and configure the AutoThrottle extension (disabled by default)
# See http://doc.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = 'httpcache'
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'

# Enables scheduling storing requests queue in redis.
SCHEDULER = "scrapy_redis.scheduler.Scheduler"

# # Ensure all spiders share same duplicates filter through redis.
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
STATS_CLASS = 'jd_comment.statscol.graphite.RedisGraphiteStatsCollector'

REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_DB="jd_good_filter"

GRAPHITE_HOST = '127.0.0.1'
GRAPHITE_PORT = 2003
STATS_KEY = "scrapy:jd_comment:stats"

```

