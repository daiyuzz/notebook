# 机器学习实战教程（四）：朴素贝叶斯基础篇之言论过滤器


## 一、前言
朴素贝叶斯算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题。该算法的优点在于简单易懂、学习效率高、在某些领域的分类问题中能够与决策树、神经网络相媲美。但是由于该算法以自变量之间的独立(条件特征独立)性和连续变量的正态性假设为前提，就会导致算法精度在某种程度上受影响。

本篇文章将从朴素贝叶斯推断原理开始讲起，通过实例进行辅助讲解。最后，使用Python编程实现一个简单的言论过滤器。


## 朴素贝叶斯理论
朴素贝叶斯是贝叶斯决策理论的一部分，所以在讲述朴素贝叶斯之前有必要快速了解一下贝叶斯决策理论。

### 1、贝叶斯决策理论
假设现在我们有一个数据集，它由两类数据组成，数据分布如下图所示：
![](assets/20190924142750192_660769268.png =513x)

我们现在用p1(x,y)表示数据点(x,y)属于类别1(图中红色圆点表示的类别)的概率，用p2(x,y)表示数据点(x,y)属于类别2(图中蓝色三角形表示的类别)的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：
- 如果p1(x,y)>p2(x,y)，那么类别为1
- 如果p1(x,y)<p2(x,y)，那么类别为2

也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。已经了解了贝叶斯决策理论的核心思想，那么接下来，就是学习如何计算p1和p2概率

### 2、条件概率
在学习计算p1 和p2概率之前，我们需要了解什么是条件概率(Conditional probability)，就是指在事件B发生的情况下，事件A发生的概率，用P(A|B)来表示。
![](assets/20190924143451981_629697278.png =398x)

![](assets/20190924143549004_1277061326.png =600x)

### 3、全概率公式
除了条件概率以外，在计算p1和p2的时候，还要用到全概率公式，因此，这里继续推导全概率公式。

假定样本空间S，是两个事件A与A'的和。

![](assets/20190924143828785_2005233104.png =500x)
上图中，红色部分是事件A，绿色部分是事件A'，它们共同构成了样本空间S。

在这种情况下，事件B可以划分成两个部分。
![](assets/20190924143926961_1374450868.png =500x)

![](assets/20190924144059985_676682601.png =900x)

### 4、贝叶斯推断
对条件概率公式进行形变，可以得到如下形式：
![](assets/20190924144301680_1405256545.png =900x)

所以，条件概率可以理解成下面的式子：
    后验概率 = 先验概率 × 调整因子

这就是贝叶斯推断的含义，我们先预估一个“先验概率”，然后加入实验结果，看这个实验到底是增强了还是削弱了“先验概率”，由此得到一个更接近事实的“后验概率”。

在这里，如果"可能性函数"P(B|A)/P(B)>1，意味着"先验概率"被增强，事件A的发生的可能性变大；如果"可能性函数"=1，意味着B事件无助于判断事件A的可能性；如果"可能性函数"<1，意味着"先验概率"被削弱，事件A的可能性变小。

为了加深对贝叶斯推断的理解，我们举一个例子。
![](assets/20190924144757858_216953068.png =500x)

两个一模一样的碗，一号碗有30颗水果糖和10颗巧克力糖，二号碗有水果糖和巧克力糖各20颗。现在随机选择一个碗，从中摸出一颗糖，发现是水果糖。请问这颗水果糖来自一号碗的概率有多大？

我们假定，H1表示一号碗，H2表示二号碗。由于这两个碗是一样的，所以P(H1)=P(H2)，也就是说，在取出水果糖之前，这两个碗被选中的概率相同。因此，P(H1)=0.5，我们把这个概率就叫做"先验概率"，即没有做实验之前，来自一号碗的概率是0.5。

再假定，E表示水果糖，所以问题就变成了在已知E的情况下，来自一号碗的概率有多大，即求P(H1|E)。我们把这个概率叫做"后验概率"，即在E事件发生之后，对P(H1)的修正。

根据条件概率公式，得到
![](assets/20190924144942163_1680346672.png =300x)

已知，P(H1)等于0.5，P(E|H1)为一号碗中取出水果糖的概率，等于30÷(30+10)=0.75，那么求出P(E)就可以得到答案。根据全概率公式，

![](assets/20190924145050943_69483427.png =900x)

**同时再思考一个问题，在使用该算法的时候，如果不需要知道具体的类别概率，即上面P(H1|E)=0.6，只需要知道所属类别，即来自一号碗，我们有必要计算P(E)这个全概率吗？要知道我们只需要比较 P(H1|E)和P(H2|E)的大小，找到那个最大的概率就可以。既然如此，两者的分母都是相同的，那我们只需要比较分子即可。即比较P(E|H1)P(H1)和P(E|H2)P(H2)的大小，所以为了减少计算量，全概率公式在实际编程中可以不使用。**


### 5、朴素贝叶斯推断

